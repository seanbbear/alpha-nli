{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chad went to get the wheel alignment measured on his car.\n",
      "The mechanic provided a working alignment with new body work.\n",
      "Chad was waiting for his car to be washed.\n",
      "Chad was waiting for his car to be finished.\n",
      "2\n",
      "Chad went to get the wheel alignment measured on his car.\n",
      "The mechanic provided a working alignment with new body work.\n",
      "Chad's car had all sorts of other problems besides alignment.\n",
      "Chad's car had all sorts of benefits other than being sexy.\n",
      "2\n",
      "Chad went to get the wheel alignment measured on his car.\n",
      "The mechanic provided a working alignment with new body work.\n",
      "Chad's mechanic said he had major alignment problems.\n",
      "Chad's mechanic said he found no problems with his car.\n",
      "2\n",
      "Chad went to get the wheel alignment measured on his car.\n",
      "The mechanic provided a working alignment with new body work.\n",
      "He ran into a pole on the way to the mechanic.\n",
      "The mechanic ran into a pole on the way to him.\n",
      "2\n",
      "Chad went to get the wheel alignment measured on his car.\n",
      "The mechanic provided a working alignment with new body work.\n",
      "the mechanic told him that he could fix his wheel.\n",
      "the mechanic told him that he could not fix his wheel.\n",
      "2\n",
      "Chad loves Barry Bonds.\n",
      "Chad ensured that he took a picture to remember the event.\n",
      "Chad went to meet Barry Bonds.\n",
      "Chad failed to meet Barry Bonds.\n",
      "2\n",
      "Chad loves Barry Bonds.\n",
      "Chad ensured that he took a picture to remember the event.\n",
      "Chad fought Barry Bonds.\n",
      "Chad met Barry Bonds.\n",
      "2\n",
      "Chad loves Barry Bonds.\n",
      "Chad ensured that he took a picture to remember the event.\n",
      "Chad got to meet Barry Bonds online, chatting.\n",
      "Chad got to meet Barry Bonds.\n",
      "2\n",
      "Chad loves Barry Bonds.\n",
      "Chad ensured that he took a picture to remember the event.\n",
      "Chad met Barry Bonds.\n",
      "Chad ignored Barry Bonds.\n",
      "2\n",
      "Chad loves Barry Bonds.\n",
      "Chad ensured that he took a picture to remember the event.\n",
      "Chad missed Barry Bonds.\n",
      "Chad met Barry Bonds.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "\n",
    "count = 0\n",
    "with jsonlines.open('train-combine.jsonl') as f:\n",
    "        for obj in f:\n",
    "            print(obj['obs1'])\n",
    "            print(obj['obs2'])\n",
    "            print(obj['hyp1'])\n",
    "            print(obj['hyp2'])\n",
    "            print(obj['label'])\n",
    "            count+=1\n",
    "            if count==10:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertForMultipleChoice(AlbertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.albert = AlbertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "#     @add_start_docstrings_to_callable(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
    "#     @add_code_sample_docstrings(\n",
    "#         tokenizer_class=_TOKENIZER_FOR_DOC,\n",
    "#         checkpoint=\"albert-base-v2\",\n",
    "#         output_type=MultipleChoiceModelOutput,\n",
    "#         config_class=_CONFIG_FOR_DOC,\n",
    "#     )\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
    "            Labels for computing the multiple choice classification loss.\n",
    "            Indices should be in ``[0, ..., num_choices-1]`` where `num_choices` is the size of the second dimension\n",
    "            of the input tensors. (see `input_ids` above)\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
    "\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
    "        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
    "        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
    "        inputs_embeds = (\n",
    "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
    "            if inputs_embeds is not None\n",
    "            else None\n",
    "        )\n",
    "        outputs = self.albert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "#         print(logits.shape)\n",
    "        reshaped_logits = logits.view(-1, num_choices)\n",
    "#         print(reshaped_logits.shape)\n",
    "#         print(logits, reshaped_logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(reshaped_logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (reshaped_logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MultipleChoiceModelOutput(\n",
    "            loss=loss,\n",
    "            logits=reshaped_logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForMultipleChoice: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForMultipleChoice were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2,   19, 1591,  ...,    0,    0,    0],\n",
      "        [   2,   32,   25,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "torch.Size([2, 512])\n",
      "{'input_ids': tensor([[[   2,   19, 1591,  ...,    0,    0,    0],\n",
      "         [   2,   32,   25,  ...,    0,    0,    0]]]), 'token_type_ids': tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
      "         [0, 0, 0,  ..., 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer,AlbertPreTrainedModel,AlbertModel\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPooling,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "import torch\n",
    "from torch import nn  \n",
    "from torch.nn import CrossEntropyLoss\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "model = AlbertForMultipleChoice.from_pretrained('albert-base-v2', return_dict=True)\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "encoding = tokenizer([[prompt, prompt], [choice0, choice1]], return_tensors='pt', padding='max_length')\n",
    "print(encoding)\n",
    "print(encoding['input_ids'].shape)\n",
    "print({k: v.unsqueeze(0) for k,v in encoding.items()})\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "# the linear classifier still needs to be trained\n",
    "# print(outputs)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "input_ids = np.zeros(shape=(2,1024))   \n",
    "print(input_ids)\n",
    "print(input_ids.reshape(2048))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [0,1,2]\n",
    "print(a[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
